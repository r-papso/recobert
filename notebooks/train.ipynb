{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rastislav_papso/.conda/envs/nlp_course/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import RecoBERT\n",
    "from src.data import CollatorWrapper, RecoDataset, RobertaTokenizerWrapper\n",
    "from src.train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "l2_reg = 0.0\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at gerulata/slovakbert were not used when initializing RobertaModel: ['lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at gerulata/slovakbert and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# SlovakBERT (https://arxiv.org/abs/2109.15254)\n",
    "tokenizer = RobertaTokenizer.from_pretrained('gerulata/slovakbert')\n",
    "tokenize_fn = RobertaTokenizerWrapper(tokenizer)\n",
    "bert = RobertaModel.from_pretrained('gerulata/slovakbert')\n",
    "collator = DataCollatorForLanguageModeling(tokenizer)\n",
    "collate_fn = CollatorWrapper(tokenize_fn, collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT (https://arxiv.org/pdf/1810.04805.pdf)\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "# bert = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "# collator = DataCollatorForLanguageModeling(tokenizer)\n",
    "# collate_fn = CollatorWrapper(tokenizer, collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/fashion.csv\", index_col=0)\n",
    "dataset = RecoDataset(df=df, swap_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = list(range(len(dataset)))\n",
    "trainval_split = int(len(dataset) * 0.6)\n",
    "valtest_split = int(len(dataset) * 0.8)\n",
    "train_idxs, val_idxs = idxs[:trainval_split], idxs[trainval_split:valtest_split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idxs)\n",
    "val_sampler = SubsetRandomSampler(val_idxs)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, collate_fn=collate_fn, num_workers=workers)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler, collate_fn=collate_fn, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecoBERT(bert, tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:3 device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(model.parameters(), lr=lr, weight_decay=l2_reg, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:51:40 - Epoch 000: Train Loss = 3.9701\n",
      "12:52:06 - Epoch 000: Val Loss = 2.1140\n",
      "12:55:08 - Epoch 001: Train Loss = 1.6882\n",
      "12:55:34 - Epoch 001: Val Loss = 1.4403\n",
      "12:58:33 - Epoch 002: Train Loss = 1.2563\n",
      "12:58:59 - Epoch 002: Val Loss = 1.1529\n",
      "13:01:58 - Epoch 003: Train Loss = 1.0352\n",
      "13:02:25 - Epoch 003: Val Loss = 1.0815\n",
      "13:05:23 - Epoch 004: Train Loss = 0.8889\n",
      "13:05:49 - Epoch 004: Val Loss = 0.9260\n",
      "13:08:48 - Epoch 005: Train Loss = 0.8336\n",
      "13:09:15 - Epoch 005: Val Loss = 0.9012\n",
      "13:12:14 - Epoch 006: Train Loss = 0.7417\n",
      "13:12:41 - Epoch 006: Val Loss = 0.8189\n",
      "13:15:40 - Epoch 007: Train Loss = 0.6823\n",
      "13:16:06 - Epoch 007: Val Loss = 0.7423\n",
      "13:19:05 - Epoch 008: Train Loss = 0.6333\n",
      "13:19:31 - Epoch 008: Val Loss = 0.7846\n",
      "13:22:30 - Epoch 009: Train Loss = 0.6197\n",
      "13:22:57 - Epoch 009: Val Loss = 0.7339\n",
      "13:25:56 - Epoch 010: Train Loss = 0.5940\n",
      "13:26:23 - Epoch 010: Val Loss = 0.6797\n",
      "13:29:22 - Epoch 011: Train Loss = 0.5619\n",
      "13:29:49 - Epoch 011: Val Loss = 0.6585\n",
      "13:32:47 - Epoch 012: Train Loss = 0.5318\n",
      "13:33:12 - Epoch 012: Val Loss = 0.6738\n",
      "13:36:12 - Epoch 013: Train Loss = 0.6148\n",
      "13:36:37 - Epoch 013: Val Loss = 0.7018\n",
      "13:39:36 - Epoch 014: Train Loss = 0.5423\n",
      "13:40:02 - Epoch 014: Val Loss = 0.6960\n",
      "13:42:59 - Epoch 015: Train Loss = 0.5036\n",
      "13:43:25 - Epoch 015: Val Loss = 0.7027\n",
      "13:46:24 - Epoch 016: Train Loss = 0.4757\n",
      "13:46:49 - Epoch 016: Val Loss = 0.6705\n",
      "13:49:47 - Epoch 017: Train Loss = 0.4434\n",
      "13:50:14 - Epoch 017: Val Loss = 0.5988\n",
      "13:53:13 - Epoch 018: Train Loss = 0.4572\n",
      "13:53:38 - Epoch 018: Val Loss = 0.6641\n",
      "13:56:37 - Epoch 019: Train Loss = 0.4397\n",
      "13:57:03 - Epoch 019: Val Loss = 0.6563\n",
      "14:00:02 - Epoch 020: Train Loss = 0.4273\n",
      "14:00:28 - Epoch 020: Val Loss = 0.6378\n",
      "14:03:27 - Epoch 021: Train Loss = 0.4318\n",
      "14:03:53 - Epoch 021: Val Loss = 0.6729\n",
      "14:06:54 - Epoch 022: Train Loss = 0.4308\n",
      "14:07:20 - Epoch 022: Val Loss = 0.6413\n",
      "14:10:18 - Epoch 023: Train Loss = 0.4154\n",
      "14:10:44 - Epoch 023: Val Loss = 0.6300\n",
      "14:13:42 - Epoch 024: Train Loss = 0.4194\n",
      "14:14:08 - Epoch 024: Val Loss = 0.6380\n",
      "14:17:08 - Epoch 025: Train Loss = 0.4041\n",
      "14:17:34 - Epoch 025: Val Loss = 0.6231\n",
      "14:20:32 - Epoch 026: Train Loss = 0.3887\n",
      "14:20:58 - Epoch 026: Val Loss = 0.7132\n",
      "14:23:58 - Epoch 027: Train Loss = 0.3954\n",
      "14:24:24 - Epoch 027: Val Loss = 0.6101\n",
      "14:27:23 - Epoch 028: Train Loss = 0.4114\n",
      "14:27:49 - Epoch 028: Val Loss = 0.6041\n",
      "14:30:48 - Epoch 029: Train Loss = 0.3782\n",
      "14:31:13 - Epoch 029: Val Loss = 0.6085\n",
      "14:34:11 - Epoch 030: Train Loss = 0.3647\n",
      "14:34:37 - Epoch 030: Val Loss = 0.6547\n",
      "14:37:35 - Epoch 031: Train Loss = 0.3557\n",
      "14:38:01 - Epoch 031: Val Loss = 0.6245\n",
      "14:41:00 - Epoch 032: Train Loss = 0.3449\n",
      "14:41:26 - Epoch 032: Val Loss = 0.6025\n",
      "14:44:27 - Epoch 033: Train Loss = 0.3637\n",
      "14:44:53 - Epoch 033: Val Loss = 0.6477\n",
      "14:47:52 - Epoch 034: Train Loss = 0.3627\n",
      "14:48:18 - Epoch 034: Val Loss = 0.6691\n",
      "14:51:16 - Epoch 035: Train Loss = 0.3382\n",
      "14:51:42 - Epoch 035: Val Loss = 0.6101\n",
      "14:54:43 - Epoch 036: Train Loss = 0.3441\n",
      "14:55:09 - Epoch 036: Val Loss = 0.7020\n",
      "14:58:09 - Epoch 037: Train Loss = 0.3389\n",
      "14:58:35 - Epoch 037: Val Loss = 0.6117\n",
      "No improvement in 20 epochs, early stopping...\n"
     ]
    }
   ],
   "source": [
    "model = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optim=optim,\n",
    "    epochs=epochs,\n",
    "    device=device,\n",
    "    checkpoint=\"./checkpoint\",\n",
    "    early_stop=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rastislav_papso/.conda/envs/nlp_course/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = torch.randn(2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0974, 0.7633, 0.4672, 1.1625, 0.4084]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0974, 0.7633, 0.4672, 1.1625, 0.4084])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('nlp_course')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cefe024ddf3585106c987cb172af2221161944a540f8557eea5f2b2b960ae866"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
