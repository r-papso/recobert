{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rastislav_papso/.conda/envs/nlp_course/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import RecoBERT\n",
    "from src.data import TrainCollator, RecoDataset\n",
    "from src.train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "l2_reg = 0.0\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "workers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SlovakBERT (https://arxiv.org/abs/2109.15254)\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('gerulata/slovakbert')\n",
    "# bert = RobertaModel.from_pretrained('gerulata/slovakbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# BERT (https://arxiv.org/pdf/1810.04805.pdf)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "bert = BertModel.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorForLanguageModeling(tokenizer)\n",
    "collate_fn = TrainCollator(tokenizer, collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines = pd.read_csv(\"../data/winemag-data-130k-v3.csv\", index_col=0)\n",
    "dataset = RecoDataset(df=wines, swap_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = list(range(len(dataset)))\n",
    "split = int(len(dataset) * 0.8)\n",
    "train_idxs, val_idxs = idxs[:split], idxs[split:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idxs)\n",
    "val_sampler = SubsetRandomSampler(val_idxs)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, collate_fn=collate_fn, num_workers=workers)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler, collate_fn=collate_fn, num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecoBERT(bert, tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:2 device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam(model.parameters(), lr=lr, weight_decay=l2_reg, betas=(beta1, beta2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:08:26 - Epoch 000: Train Loss = 2.9264\n",
      "11:09:38 - Epoch 000: Val Loss = 1.9746\n",
      "11:21:41 - Epoch 001: Train Loss = 1.8581\n",
      "11:22:53 - Epoch 001: Val Loss = 1.6117\n",
      "11:34:58 - Epoch 002: Train Loss = 1.6205\n",
      "11:36:10 - Epoch 002: Val Loss = 1.4515\n",
      "11:48:14 - Epoch 003: Train Loss = 1.4803\n",
      "11:49:26 - Epoch 003: Val Loss = 1.3901\n",
      "12:01:29 - Epoch 004: Train Loss = 1.4041\n",
      "12:02:41 - Epoch 004: Val Loss = 1.2861\n",
      "12:14:45 - Epoch 005: Train Loss = 1.3390\n",
      "12:15:57 - Epoch 005: Val Loss = 1.2436\n",
      "12:28:01 - Epoch 006: Train Loss = 1.2914\n",
      "12:29:14 - Epoch 006: Val Loss = 1.2027\n",
      "12:41:18 - Epoch 007: Train Loss = 1.2588\n",
      "12:42:30 - Epoch 007: Val Loss = 1.1759\n",
      "12:54:35 - Epoch 008: Train Loss = 1.2266\n",
      "12:55:47 - Epoch 008: Val Loss = 1.1610\n",
      "13:07:51 - Epoch 009: Train Loss = 1.1932\n",
      "13:09:03 - Epoch 009: Val Loss = 1.1230\n",
      "13:21:09 - Epoch 010: Train Loss = 1.1806\n",
      "13:22:22 - Epoch 010: Val Loss = 1.1095\n",
      "13:34:30 - Epoch 011: Train Loss = 1.1593\n",
      "13:35:42 - Epoch 011: Val Loss = 1.0880\n",
      "13:47:49 - Epoch 012: Train Loss = 1.1387\n",
      "13:49:02 - Epoch 012: Val Loss = 1.0758\n",
      "14:01:13 - Epoch 013: Train Loss = 1.1196\n",
      "14:02:25 - Epoch 013: Val Loss = 1.0759\n",
      "14:14:33 - Epoch 014: Train Loss = 1.1079\n",
      "14:15:46 - Epoch 014: Val Loss = 1.0749\n",
      "14:27:56 - Epoch 015: Train Loss = 1.1204\n",
      "14:29:09 - Epoch 015: Val Loss = 1.0399\n",
      "14:41:21 - Epoch 016: Train Loss = 1.0828\n",
      "14:42:33 - Epoch 016: Val Loss = 1.0509\n",
      "14:54:42 - Epoch 017: Train Loss = 1.0772\n",
      "14:55:54 - Epoch 017: Val Loss = 1.0314\n",
      "15:08:02 - Epoch 018: Train Loss = 1.0674\n",
      "15:09:14 - Epoch 018: Val Loss = 1.0329\n",
      "15:21:24 - Epoch 019: Train Loss = 1.0633\n",
      "15:22:36 - Epoch 019: Val Loss = 1.0236\n",
      "15:34:45 - Epoch 020: Train Loss = 1.0501\n",
      "15:35:57 - Epoch 020: Val Loss = 1.0134\n",
      "15:48:07 - Epoch 021: Train Loss = 1.0348\n",
      "15:49:19 - Epoch 021: Val Loss = 0.9942\n",
      "16:01:28 - Epoch 022: Train Loss = 1.0294\n",
      "16:02:40 - Epoch 022: Val Loss = 0.9990\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m train(\n\u001b[1;32m      2\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      3\u001b[0m     train_loader\u001b[39m=\u001b[39;49mtrain_loader,\n\u001b[1;32m      4\u001b[0m     val_loader\u001b[39m=\u001b[39;49mval_loader,\n\u001b[1;32m      5\u001b[0m     optim\u001b[39m=\u001b[39;49moptim,\n\u001b[1;32m      6\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m      7\u001b[0m     device\u001b[39m=\u001b[39;49mdevice,\n\u001b[1;32m      8\u001b[0m     checkpoint\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m./checkpoint\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      9\u001b[0m     early_stop\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m\n\u001b[1;32m     10\u001b[0m )\n",
      "File \u001b[0;32m~/projects/nlp_course/recobert/notebooks/../src/train.py:77\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optim, epochs, device, checkpoint, early_stop)\u001b[0m\n\u001b[1;32m     74\u001b[0m ce_loss \u001b[39m=\u001b[39m ce_loss_fn\u001b[39m.\u001b[39mforward(y[\u001b[39m\"\u001b[39m\u001b[39mlm_scores\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m), mlm_label)\n\u001b[1;32m     75\u001b[0m loss \u001b[39m=\u001b[39m be_loss \u001b[39m+\u001b[39m ce_loss\n\u001b[0;32m---> 77\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     78\u001b[0m optim\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     79\u001b[0m sum_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.conda/envs/nlp_course/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/nlp_course/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optim=optim,\n",
    "    epochs=epochs,\n",
    "    device=device,\n",
    "    checkpoint=\"./checkpoint\",\n",
    "    early_stop=5\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('nlp_course')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cefe024ddf3585106c987cb172af2221161944a540f8557eea5f2b2b960ae866"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
